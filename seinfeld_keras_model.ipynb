{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1ba62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9790b47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = pd.read_csv('seinfeld_data/scripts.csv')\n",
    "scripts['line'] = scripts.Character + ': ' + scripts.Dialogue\n",
    "corpus = scripts.line.astype(str).tolist()\n",
    "corpus=''.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1de5ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(corpus))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99eebbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = tf.device('device:CPU:0') \n",
    "\n",
    "chars = tf.strings.unicode_split(vocab, input_encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "487f3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0436c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "chars = chars_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30389cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9701d65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(corpus, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2730b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'JERRY: Do you know what this is all about? Do you know, why were here? To be out, this is out...and '\n",
      "Target: b'ERRY: Do you know what this is all about? Do you know, why were here? To be out, this is out...and o'\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8c9970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a89fa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f01bfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units, timesteps, features):\n",
    "      super().__init__(self)\n",
    "      self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "      self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True)\n",
    "      self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "      # Set the values of the timesteps and features attributes\n",
    "      self.timesteps = timesteps\n",
    "      self.features = features\n",
    "\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "  # Reshape the input tensor to have the correct shape for the GRU layer\n",
    "      x = tf.reshape(inputs, ( self.timesteps, self.features))\n",
    "\n",
    "      x = self.embedding(x, training=training)\n",
    "      if states is None:\n",
    "        states = self.gru.get_initial_state(x)\n",
    "      x, states = self.gru(x, initial_state=states, training=training)\n",
    "      x = self.dense(x, training=training)\n",
    "\n",
    "      if return_state:\n",
    "        return x, states\n",
    "      else:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6abd46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units, \n",
    "    timesteps=64, \n",
    "    features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35f113ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77b7f71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 95) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35b306f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9c36ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'hter. My father asked me to come here and pick up his jacket for him.LANDLORD: Oh, hello Miss Pepper'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'T$[UNK]/#\\xc2\\x924[UNK]A>3vy]%8#Q)]5}7,ad~Hv\\xc2\\x9294/[)#ji8qB,6P?-6+<X+`STRBZDKh\\xc2\\x92:K6mo#3\"w?FvW~SBa8W*{.TPb9$(7\\\\$C\\xc2\\x91QIL}UB'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f07cce99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 95)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.555552, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "55411216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.15927"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3874b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './seinfeld_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dedfc7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 22:26:30.844574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-19 22:26:31.339569: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-12-19 22:26:31.381185: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x169bde320\n",
      "2022-12-19 22:26:31.381585: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:418 : NOT_FOUND: could not find registered platform with id: 0x169bde320\n",
      "2022-12-19 22:26:31.383281: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_5' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 530, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/3p/xpgnq5_d0cz13g99y3v4g03r0000gn/T/ipykernel_42748/2303347571.py\", line 2, in <module>\n      history = model.fit(dataset, epochs=EPOCHS)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_5'\ncould not find registered platform with id: 0x169bde320\n\t [[{{node StatefulPartitionedCall_5}}]] [Op:__inference_train_function_5052]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_5' defined at (most recent call last):\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 711, in start\n      self.io_loop.start()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 411, in do_execute\n      res = shell.run_cell(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 530, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in run_cell\n      result = self._run_cell(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2995, in _run_cell\n      return runner(coro)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3194, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3373, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3433, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/3p/xpgnq5_d0cz13g99y3v4g03r0000gn/T/ipykernel_42748/2303347571.py\", line 2, in <module>\n      history = model.fit(dataset, epochs=EPOCHS)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/engine/training.py\", line 1027, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 527, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1140, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 634, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1166, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1216, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/opt/anaconda3/envs/tensorflow/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py\", line 1211, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_5'\ncould not find registered platform with id: 0x169bde320\n\t [[{{node StatefulPartitionedCall_5}}]] [Op:__inference_train_function_5052]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit(dataset, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f5fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf. __version__) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
